{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd62d9-5640-4b79-b809-16521f16677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# fetch dataset \n",
    "rice_cammeo_and_osmancik = fetch_ucirepo(id=545) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = rice_cammeo_and_osmancik.data.features \n",
    "y = rice_cammeo_and_osmancik.data.targets \n",
    " \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a single plot for all features\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot box plots for each scaled feature\n",
    "sns.boxplot(data=X)\n",
    "plt.title('Distribution of Scaled Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "\n",
    "\n",
    "# Save the plot directly from the Figure object\n",
    "fig = plt.gcf()  # Get the current figure\n",
    "fig.savefig('boxplot12.png')  # Save the figure as a PNG file\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Features')\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine the scaled training features into a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a single plot for all features\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot box plots for each scaled feature\n",
    "sns.boxplot(data=X_scaled_df)\n",
    "plt.title('Distribution of Scaled Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "\n",
    "\n",
    "# Save the plot directly from the Figure object\n",
    "fig = plt.gcf()  # Get the current figure\n",
    "fig.savefig('boxplot.png')  # Save the figure as a PNG file\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = X_scaled_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Features')\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "y = np.ravel(y)\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Make model\n",
    "modelSVM = SVC(C= 1, kernel= 'linear', probability=True)\n",
    "\n",
    "#train model\n",
    "modelSVM.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions for test data\n",
    "y_pred = modelSVM.predict(X_test)\n",
    "\n",
    "# Convert test data and predictions to DataFrame for easier visualization\n",
    "test_results = pd.DataFrame(X_test, columns=rice_cammeo_and_osmancik.feature_names)\n",
    "test_results['Actual'] = y_test\n",
    "test_results['Predicted'] = y_pred\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Display test results as a table\n",
    "display(test_results)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'actual_values' contains the actual values and 'predicted_values' contains the predicted values\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(range(len(y_test)), y_test, color='blue', label='Actual', marker='*',s=60)\n",
    "plt.scatter(range(len(y_pred)), y_pred, color='red', label='Predicted', marker='s', s=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# prediction and accuray\n",
    "modelSVM.score(X_test,y_test)\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Assuming y_pred contains the predicted labels and y_test contains the true labels\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df = pd.DataFrame(report)\n",
    "\n",
    "# Transpose the DataFrame\n",
    "report_df = report_df.transpose()\n",
    "\n",
    "# Round the values to two decimal points\n",
    "report_df = report_df.round(2)\n",
    "\n",
    "# Display the transposed report\n",
    "print(report_df.to_string())\n",
    "\n",
    "# Display test results as a table\n",
    "display(report_df)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract true positives, true negatives, false positives, and false negatives\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"\\nSensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# choose optimal hyperparamters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10]},\n",
    "    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1]},\n",
    "    {'kernel': ['sigmoid'], 'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1]},\n",
    "]\n",
    "\n",
    "# Create the SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Accuracy with Best Parameters:\", accuracy)\n",
    "\n",
    "\n",
    "# predict unseen data\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'best_model' is your trained SVM model\n",
    "# 'sample_input' is a single sample input (a list or a 1D array)\n",
    "# 'actual_label' is the actual label corresponding to the sample input\n",
    "\n",
    "actual_label = 'Cammeo'\n",
    "\n",
    "# Create a single sample input with 7 features\n",
    "sample_input = [15231, 525.578979, 229.749878, 85.093788, 0.928882, 15617, 0.572896]\n",
    "\n",
    "\n",
    "\n",
    "# Convert the sample input to a numpy array\n",
    "sample_input_array = np.array(sample_input)\n",
    "\n",
    "# Reshape the input to a 2D array with one row\n",
    "sample_input_reshaped = sample_input_array.reshape(1, -1)\n",
    "\n",
    "# Make predictions for the single sample input\n",
    "predicted_label = best_model.predict(sample_input_reshaped)\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"Predicted label:\", predicted_label)\n",
    "\n",
    "# Print the actual label\n",
    "print(\"Actual label:\", actual_label)\n",
    "\n",
    "# Compare the predicted label with the actual label\n",
    "if predicted_label == actual_label:\n",
    "    print(\"Prediction is correct!\")\n",
    "else:\n",
    "    print(\"Prediction is incorrect.\")\n",
    "\n",
    "\n",
    "# find range of each feature\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X contains your feature data and y contains your target labels\n",
    "\n",
    "# Create a DataFrame combining X and y\n",
    "data = pd.DataFrame(X, columns=['Area', 'Perimeter', 'Major_Axis_Length', 'Minor_Axis_Length', 'Eccentricity', 'Convex_Area', 'Extent'])\n",
    "data['Class'] = y\n",
    "\n",
    "# Calculate range for each feature for each class\n",
    "class_ranges = {}\n",
    "classes = data['Class'].unique()\n",
    "\n",
    "for class_label in classes:\n",
    "    class_data = data[data['Class'] == class_label]\n",
    "    class_ranges[class_label] = {\n",
    "        'Area': (class_data['Area'].min(), class_data['Area'].max()),\n",
    "        'Perimeter': (class_data['Perimeter'].min(), class_data['Perimeter'].max()),\n",
    "        'Major_Axis_Length': (class_data['Major_Axis_Length'].min(), class_data['Major_Axis_Length'].max()),\n",
    "        'Minor_Axis_Length': (class_data['Minor_Axis_Length'].min(), class_data['Minor_Axis_Length'].max()),\n",
    "        'Eccentricity': (class_data['Eccentricity'].min(), class_data['Eccentricity'].max()),\n",
    "        'Convex_Area': (class_data['Convex_Area'].min(), class_data['Convex_Area'].max()),\n",
    "        'Extent': (class_data['Extent'].min(), class_data['Extent'].max())\n",
    "    }\n",
    "\n",
    "# Print the ranges for each feature for each class\n",
    "for class_label, ranges in class_ranges.items():\n",
    "    print(f\"Class: {class_label}\")\n",
    "    for feature, range_values in ranges.items():\n",
    "        print(f\"{feature} Range: {range_values}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# a code to predict bsed upon range\n",
    "def predict_class(data_point, class_ranges):\n",
    "    scores = {}\n",
    "    for class_label, ranges in class_ranges.items():\n",
    "        score = 0\n",
    "        for feature, value in data_point.items():\n",
    "            if ranges[feature][0] <= value <= ranges[feature][1]:\n",
    "                score += 1\n",
    "        scores[class_label] = score\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Actual ranges for each feature for both classes\n",
    "class_ranges = {\n",
    "    'Cammeo': {\n",
    "        'Area': (9908, 18913),\n",
    "        'Perimeter': (410.506012, 548.4459839),\n",
    "        'Major_Axis_Length': (170.7816467, 239.010498),\n",
    "        'Minor_Axis_Length': (67.69534302, 107.54245),\n",
    "        'Eccentricity': (0.83743304, 0.948006928),\n",
    "        'Convex_Area': (10205, 19099),\n",
    "        'Extent': (0.49741286, 0.861049533)\n",
    "    },\n",
    "    'Osmancik': {\n",
    "        'Area': (7551, 15420),\n",
    "        'Perimeter': (359.1000061, 503.4599915),\n",
    "        'Major_Axis_Length': (145.2644653, 209.6511688),\n",
    "        'Minor_Axis_Length': (59.53240585, 101.7622604),\n",
    "        'Eccentricity': (0.777232587, 0.935528278),\n",
    "        'Convex_Area': (7723, 15800),\n",
    "        'Extent': (0.501078486, 0.83274734)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example new data point\n",
    "new_data_point = {\n",
    "    'Area': 10000,\n",
    "    'Perimeter': 450,\n",
    "    'Major_Axis_Length': 180,\n",
    "    'Minor_Axis_Length': 75,\n",
    "    'Eccentricity': 0.85,\n",
    "    'Convex_Area': 11000,\n",
    "    'Extent': 0.65\n",
    "}\n",
    "\n",
    "# Predict the class for the new data point\n",
    "predicted_class = predict_class(new_data_point, class_ranges)\n",
    "print(\"Predicted Class:\", predicted_class)\n",
    "\n",
    "\n",
    "# Predict classes for scaled test data using the trained SVM model\n",
    "\n",
    "modelSVM.score(X_test,y_test)\n",
    "\n",
    "# which feature has more discriminating power\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have trained your SVM model and stored it in 'modelSVM'\n",
    "\n",
    "# Extract the coefficients (weights) of the SVM model\n",
    "coefficients = modelSVM.coef_\n",
    "\n",
    "# Get the absolute values of the coefficients to understand the importance of each feature\n",
    "absolute_coefficients = np.abs(coefficients)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = ['Area', 'Perimeter', 'Major_Axis_Length', 'Minor_Axis_Length', 'Eccentricity', 'Convex_Area', 'Extent']\n",
    "\n",
    "# Create a dictionary to store feature importance\n",
    "feature_importance = dict(zip(feature_names, absolute_coefficients[0]))\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_feature_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"Feature Importance:\")\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have trained your SVM model and stored it in 'modelSVM'\n",
    "\n",
    "# Predict classes for test data using the trained SVM model\n",
    "predicted_labels_svm = modelSVM.predict(X_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, predicted_labels_svm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=['Cammeo', 'Osmancik'], yticklabels=['Cammeo', 'Osmancik'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# True Positives (TP): Diagonal elements of the confusion matrix for each class\n",
    "TP = np.diag(cm)\n",
    "\n",
    "# False Positives (FP): Sum of column values in the confusion matrix, excluding the diagonal elements, for each predicted class\n",
    "FP = np.sum(cm, axis=0) - TP\n",
    "\n",
    "# False Negatives (FN): Sum of row values in the confusion matrix, excluding the diagonal elements, for each true class\n",
    "FN = np.sum(cm, axis=1) - TP\n",
    "\n",
    "# True Negatives (TN): Sum of all values in the confusion matrix, excluding the row and column corresponding to the true and predicted classes\n",
    "TN = np.sum(cm) - (TP + FP + FN)\n",
    "\n",
    "# Print the calculated values\n",
    "for i in range(len(classes)):\n",
    "    print(f\"Class: {classes[i]}\")\n",
    "    print(f\"True Positives (TP): {TP[i]}\")\n",
    "    print(f\"False Positives (FP): {FP[i]}\")\n",
    "    print(f\"True Negatives (TN): {TN[i]}\")\n",
    "    print(f\"False Negatives (FN): {FN[i]}\\n\")\n",
    "\n",
    "# Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "# Sensitivity (True Positive Rate): TP / (TP + FN)\n",
    "sensitivity = TP / (TP + FN)\n",
    "\n",
    "# Specificity (True Negative Rate): TN / (TN + FP)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# F1 Score: 2 * (precision * recall) / (precision + recall)\n",
    "precision = TP / (TP + FP)\n",
    "recall = sensitivity\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# False Positive Rate (FPR): FP / (FP + TN)\n",
    "fpr = FP / (FP + TN)\n",
    "\n",
    "# True Positive Rate (TPR): TP / (TP + FN)\n",
    "tpr = sensitivity\n",
    "\n",
    "# Print the calculated metrics\n",
    "for i in range(len(classes)):\n",
    "    print(f\"Class: {classes[i]}\")\n",
    "    print(f\"Accuracy: {accuracy[i]}\")\n",
    "    print(f\"Sensitivity (True Positive Rate): {sensitivity[i]}\")\n",
    "    print(f\"Specificity (True Negative Rate): {specificity[i]}\")\n",
    "    print(f\"F1 Score: {f1_score[i]}\")\n",
    "    print(f\"False Positive Rate (FPR): {fpr[i]}\")\n",
    "    print(f\"True Positive Rate (TPR): {tpr[i]}\\n\")\n",
    "\n",
    "\n",
    "# Combine TP, FP, TN, FN values for both classes\n",
    "total_TP = np.sum(TP)\n",
    "total_FP = np.sum(FP)\n",
    "total_TN = np.sum(TN)\n",
    "total_FN = np.sum(FN)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = (total_TP + total_TN) / (total_TP + total_TN + total_FP + total_FN)\n",
    "\n",
    "# Overall precision, recall (True Positive Rate), F1 score\n",
    "overall_precision = total_TP / (total_TP + total_FP)\n",
    "overall_recall = total_TP / (total_TP + total_FN)\n",
    "overall_f1_score = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall)\n",
    "\n",
    "# Overall False Positive Rate (FPR)\n",
    "overall_fpr = total_FP / (total_FP + total_TN)\n",
    "\n",
    "# Overall True Positive Rate (TPR)\n",
    "overall_tpr = overall_recall\n",
    "\n",
    "# Print the overall metrics\n",
    "print(\"Overall Metrics (Combining Both Classes):\")\n",
    "print(f\"Accuracy: {overall_accuracy}\")\n",
    "print(f\"Precision: {overall_precision}\")\n",
    "print(f\"Recall (True Positive Rate): {overall_recall}\")\n",
    "print(f\"F1 Score: {overall_f1_score}\")\n",
    "print(f\"False Positive Rate (FPR): {overall_fpr}\")\n",
    "print(f\"True Positive Rate (TPR): {overall_tpr}\\n\")\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert class labels to binary format (0 and 1)\n",
    "y_test_binary = (y_test == 'Osmancik').astype(int)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, y_proba_positive)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
